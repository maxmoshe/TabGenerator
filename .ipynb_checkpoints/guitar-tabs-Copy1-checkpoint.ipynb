{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64cabc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "#Add running from cmd.\n",
    "\n",
    "SEQ_LEN = 40\n",
    "BATCH_SIZE = 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a9563be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_urls(query, pages=1, get_top_only=True):\n",
    "    urls = []\n",
    "    for page in range(1, pages+1):\n",
    "        url = f'https://www.ultimate-guitar.com/search.php?page={page}&title={query.replace(\" \", \"%20\")}&type=200'\n",
    "        r = requests.get(url)\n",
    "        if r.status_code == 404: return urls #finished iterating over all pages.\n",
    "        soup = BeautifulSoup(r.text)\n",
    "        j = soup.find(\"div\", {\"class\": \"js-store\"})['data-content']\n",
    "        tabs = json.loads(j)['store']['page']['data']['results']\n",
    "\n",
    "        song_id = 0\n",
    "        for tab in tabs:\n",
    "            if 'marketing_type' in tab: continue #ignore paid tabs.\n",
    "            if not get_top_only: urls.append(tab['tab_url']); continue\n",
    "            if song_id == 0: song_id, rating, url = tab['song_id'], tab['rating'], tab['tab_url']\n",
    "                \n",
    "            if song_id != tab['song_id']:\n",
    "                if rating > 3: #don't append bad tabs.\n",
    "                    urls.append(url)\n",
    "                    print(f'Best: {rating} - {url}')\n",
    "                song_id = tab['song_id']\n",
    "                rating = tab['rating']\n",
    "                url = tab['tab_url']\n",
    "\n",
    "            if tab['rating'] > rating:\n",
    "                rating = tab['rating']\n",
    "                url = tab['tab_url']\n",
    "                \n",
    "    return urls\n",
    "\n",
    "def get_urls_top(): #gets top 100 tabs.\n",
    "    urls = []\n",
    "    url = 'https://www.ultimate-guitar.com/top/tabs?order=hitsdailygroup_desc&type=tab'\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text)\n",
    "    j = soup.find(\"div\", {\"class\": \"js-store\"})['data-content']\n",
    "    tabs = json.loads(j)['store']['page']['data']['tabs']#['results']\n",
    "    print(tabs)\n",
    "\n",
    "    for tab in tabs:\n",
    "        if 'marketing_type' in tab: continue #paid tabs.\n",
    "        urls.append(tab['tab_url'])\n",
    "    return urls\n",
    "\n",
    "queries = ['pink floyd', 'beatles', 'jimi hendrix' , 'arctic monkeys', 'radiohead', 'fall out boy', 'black sabbath']\n",
    "#urls = get_urls('pink floyd', 10)\n",
    "#urls = get_urls_top(2)\n",
    "\n",
    "urls = []\n",
    "#urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87511aa6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_tab(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    j = soup.body.find_all(\"div\")[2]['data-content']\n",
    "    text = json.loads(j)['store']['page']['data'][\"tab_view\"]['wiki_tab']['content']\n",
    "    return text\n",
    "\n",
    "for url in urls:\n",
    "    time.sleep(2) #spamming requests blocks you.\n",
    "    text = get_tab(url)\n",
    "    name = url.split('/')[-1]\n",
    "    with open(f\"tabs/{name}\", 'w') as f:\n",
    "        try:\n",
    "            f.write(text)\n",
    "        except: continue\n",
    "    print(f'{name} downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25534961",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "import os\n",
    "\n",
    "\n",
    "tab_dict = {'-': 1, '1': 2, '2': 3, '0': 4, '7': 5, '5': 6, '4': 7, '3': 8, '9': 9, 'x': 10, '6': 11, 'b': 12, '(': 13, ')': 14, '8': 15, '/': 16, '~': 17, 'p': 18, 'h': 19, '=': 20, '\\\\': 21, 's': 22, \"'\": 23, 'r': 24}\n",
    "seq_dict = {0: 'O', 1: '-', 2: '1', 3: '2', 4: '0', 5: '7', 6: '5', 7: '4', 8: '3', 9: '9', 10: 'x', 11: '6', 12: 'b', 13: '(', 14: ')', 15: '8', 16: '/', 17: '~', 18: 'p', 19: 'h', 20: '=', 21: '\\\\', 22: 's', 23: \"'\", 24: 'r'}\n",
    "CHAR_NUM = len(tab_dict) + 1\n",
    "\n",
    "def one_hot(arr):\n",
    "    encoded = np.zeros((len(arr), len(tab_dict)+1), dtype='bool')\n",
    "    for i, n in enumerate(arr):\n",
    "        encoded[i][n] = 1\n",
    "    return encoded\n",
    "\n",
    "\n",
    "class TabGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, data_path='data', batch_size=BATCH_SIZE, shuffle=True):\n",
    "        self.timesteps = os.path.getsize('data') // 6 #number of bytes in dataset/6 = timesteps.\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.indices = np.arange(self.timesteps - SEQ_LEN)\n",
    "        if self.shuffle: np.random.shuffle(self.indices)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.timesteps - SEQ_LEN)//self.batch_size\n",
    "\n",
    "    def __getitem__(self, index): #get batch\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        x = [] #x batch/seq/6*chars\n",
    "        #y 6/batch/chars\n",
    "        y = []; y2 = []; y3 = []; y4 = []; y5 = []; y6 = [];\n",
    "        for i in indices:\n",
    "            ix, iy = self.read_seq(i)\n",
    "            x.append(ix)\n",
    "            #y.append(iy) + return x,y doesn't work for some reason (for multi output generator/keras)\n",
    "            y.append(iy[0]); y2.append(iy[1]); y3.append(iy[2]) ;y4.append(iy[3]); y5.append(iy[4]); y6.append(iy[5])\n",
    "        return np.array(x), {'output1': np.array(y), 'output2': np.array(y2), 'output3': np.array(y3), 'output4': np.array(y4), 'output5': np.array(y5), 'output6': np.array(y6)} \n",
    "\n",
    "                    \n",
    "    def read_seq(self, index): #reads, unpacks and one-hots.\n",
    "        with open('data', 'rb') as f:\n",
    "            f.seek(index)\n",
    "            d = f.read(6*SEQ_LEN)\n",
    "            unpacked = struct.unpack(f'{6*SEQ_LEN}B', d)\n",
    "            seq = one_hot(unpacked) #6*SEQ_LEN/chars > SEQ_LEN/6*\n",
    "            seq = seq.reshape(SEQ_LEN, 6*CHAR_NUM)\n",
    "            x = seq[:-1]\n",
    "            y = seq[-1].reshape(6, -1)\n",
    "            return x, y\n",
    "\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle: np.random.shuffle(self.indices)\n",
    "        \n",
    "#Seperates tab from other notation\n",
    "def seperate_tabs(text):\n",
    "    tabs = []\n",
    "    count = 0\n",
    "    text = text.replace('[tab]', '').replace('[/tab]', '').replace(' ', '')\n",
    "    text_split = text.split('\\n')\n",
    "    for i, l in enumerate(text_split):\n",
    "        if len(l) < 10: count = 0; continue\n",
    "        if l[0] == '|' and l[1] in '-123456789(': #if string is not specified, label the correct string (assuming EADGBe)\n",
    "            if count == 6: return #Tabs are for 7+ strings. Disregard the tab.\n",
    "            string = 'eBGDAE'[count]\n",
    "            count += 1\n",
    "            l = string + l\n",
    "            \n",
    "        else: count = 0\n",
    "        \n",
    "        if l[0].lower() in 'ebgda' and l[1] == '|':\n",
    "            if l[0] == 'E' and i>0 and len(text_split[i-1])>0 and text_split[i-1][0] not in 'A|': l = 'e' + l[1:] #sometimes people label high e as E. lowercase it.\n",
    "            stripped_line = l.replace('|', '').replace('\\r', '').replace(' ', '')\n",
    "            tabs.append(stripped_line)\n",
    "    return tabs\n",
    "\n",
    "\n",
    "def pad_tabs(tabs, end_padding=0):\n",
    "    longest = max([len(l) for l in tabs])\n",
    "    tabs = [l + '-' * (longest-len(l)+end_padding) for l in tabs]\n",
    "    return tabs\n",
    "\n",
    "\n",
    "\n",
    "#Concatenates tabs by common string.\n",
    "def join_tabs(tabs):\n",
    "    strings = {}\n",
    "    for l in tabs:\n",
    "        if l[0] not in strings:\n",
    "            strings[l[0]] = ''\n",
    "            continue\n",
    "        strings[l[0]] += l[1:]\n",
    "    tab_list = list(strings.values())\n",
    "    tab_list = pad_tabs(tab_list, SEQ_LEN)\n",
    "\n",
    "    return tab_list\n",
    "\n",
    "text_to_seq = lambda text: [tab_dict.get(s.lower()) or 0 for s in text] #0 is out of vocab.\n",
    "seq_to_text = lambda seq: [seq_dict[n] for n in seq]\n",
    "\n",
    "def tab_to_seq(tab):\n",
    "    for i, s in enumerate(tab):\n",
    "        tab[i] = text_to_seq(s)\n",
    "    return tab\n",
    "\n",
    "\n",
    "def write_tab_binary(tab_list): #custom file format: byte = tab-char index, 6bytes = timestep.\n",
    "    with open('data', 'ab') as f:\n",
    "        for i in range(len(tab_list[0])):\n",
    "            binary = struct.pack('6B', tab_list[0][i], tab_list[1][i], tab_list[2][i], tab_list[3][i], tab_list[4][i], tab_list[5][i])\n",
    "            f.write(binary)\n",
    "        \n",
    "\n",
    "tab_lists = []\n",
    "for filename in glob.glob('tabs/*'):\n",
    "    with open(filename, 'r') as f:\n",
    "        try:\n",
    "            text = f.read()\n",
    "        except: continue\n",
    "    tabs = seperate_tabs(text)\n",
    "    if not tabs: continue\n",
    "    tab_list = join_tabs(tabs)\n",
    "    if len(tab_list) != 6 or len(tab_list[0]) < SEQ_LEN+1: continue #if string_num isn't 6 or tab length is shorter than 1 sequence__lem, skip tab\n",
    "    tab_list = tab_to_seq(tab_list)\n",
    "    write_tab_binary(tab_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daa38814",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tab_generator\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 39, 150)]    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_25 (LSTM)                  (None, 39, 256)      416768      input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_26 (LSTM)                  (None, 256)          525312      lstm_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 512)          131584      lstm_26[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 512)          0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 512)          262656      leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 512)          0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "output1 (Dense)                 (None, 25)           12825       leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output2 (Dense)                 (None, 25)           12825       leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output3 (Dense)                 (None, 25)           12825       leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output4 (Dense)                 (None, 25)           12825       leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output5 (Dense)                 (None, 25)           12825       leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "output6 (Dense)                 (None, 25)           12825       leaky_re_lu_24[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 1,413,270\n",
      "Trainable params: 1,413,270\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "9426/9426 [==============================] - 190s 20ms/step - loss: 2.1663 - output1_loss: 0.3393 - output2_loss: 0.3563 - output3_loss: 0.3645 - output4_loss: 0.3671 - output5_loss: 0.3691 - output6_loss: 0.37004s - loss: 2.1737 - output1_loss: 0.3409 - output2_loss: 0.3577 - output3_loss: \n",
      "Epoch 2/20\n",
      "9426/9426 [==============================] - 187s 20ms/step - loss: 1.7006 - output1_loss: 0.2568 - output2_loss: 0.2766 - output3_loss: 0.2864 - output4_loss: 0.2899 - output5_loss: 0.2938 - output6_loss: 0.2972\n",
      "Epoch 3/20\n",
      "9426/9426 [==============================] - 185s 20ms/step - loss: 1.5295 - output1_loss: 0.2306 - output2_loss: 0.2479 - output3_loss: 0.2571 - output4_loss: 0.2605 - output5_loss: 0.2644 - output6_loss: 0.26903s - loss: 1.5301 - output1_loss: 0.2306 - output2_loss: 0.2479 - output3_loss: 0.2571 - output4_loss: 0.2607 - output5_loss:  - ETA: 1s - loss: 1.5297 - output1_loss: 0.2307 - output2_loss: 0.2478 - output3_loss: 0.2571 - output4_loss: 0.2606 - output5_loss: \n",
      "Epoch 4/20\n",
      "9426/9426 [==============================] - 189s 20ms/step - loss: 1.4165 - output1_loss: 0.2143 - output2_loss: 0.2292 - output3_loss: 0.2375 - output4_loss: 0.2408 - output5_loss: 0.2445 - output6_loss: 0.25024s - loss: 1.4172 - output1_loss: 0.2144 - output2_loss: 0.2293 - output3_loss: 0.2374\n",
      "Epoch 5/20\n",
      "9426/9426 [==============================] - 190s 20ms/step - loss: 1.3298 - output1_loss: 0.2016 - output2_loss: 0.2152 - output3_loss: 0.2226 - output4_loss: 0.2253 - output5_loss: 0.2295 - output6_loss: 0.2356\n",
      "Epoch 6/20\n",
      "9426/9426 [==============================] - 181s 19ms/step - loss: 1.2582 - output1_loss: 0.1910 - output2_loss: 0.2033 - output3_loss: 0.2105 - output4_loss: 0.2128 - output5_loss: 0.2168 - output6_loss: 0.2238\n",
      "Epoch 7/20\n",
      "9426/9426 [==============================] - 180s 19ms/step - loss: 1.3543 - output1_loss: 0.2039 - output2_loss: 0.2177 - output3_loss: 0.2280 - output4_loss: 0.2331 - output5_loss: 0.2333 - output6_loss: 0.2382\n",
      "Epoch 8/20\n",
      "9426/9426 [==============================] - 180s 19ms/step - loss: 1.1714 - output1_loss: 0.1779 - output2_loss: 0.1887 - output3_loss: 0.1954 - output4_loss: 0.1983 - output5_loss: 0.2016 - output6_loss: 0.20948\n",
      "Epoch 9/20\n",
      "9426/9426 [==============================] - 180s 19ms/step - loss: 1.1472 - output1_loss: 0.1749 - output2_loss: 0.1855 - output3_loss: 0.1910 - output4_loss: 0.1932 - output5_loss: 0.1972 - output6_loss: 0.2053\n",
      "Epoch 10/20\n",
      "9426/9426 [==============================] - 182s 19ms/step - loss: 1.0994 - output1_loss: 0.1685 - output2_loss: 0.1776 - output3_loss: 0.1825 - output4_loss: 0.1848 - output5_loss: 0.1889 - output6_loss: 0.1971\n",
      "Epoch 11/20\n",
      "9426/9426 [==============================] - 183s 19ms/step - loss: 1.0563 - output1_loss: 0.1621 - output2_loss: 0.1706 - output3_loss: 0.1749 - output4_loss: 0.1771 - output5_loss: 0.1814 - output6_loss: 0.1902\n",
      "Epoch 12/20\n",
      "9426/9426 [==============================] - 190s 20ms/step - loss: 1.0170 - output1_loss: 0.1569 - output2_loss: 0.1641 - output3_loss: 0.1679 - output4_loss: 0.1704 - output5_loss: 0.1743 - output6_loss: 0.18356s - loss: 1.0165 - output1_loss: 0.1568 - output2_loss: 0.1640 - output3_loss: 0.1678 - output4_loss: 0.1 - ETA: 3s - loss: 1.0168 - output1_loss: 0.1568 - output2_loss: 0.1641 - output3_loss: 0.1679 - output4_los\n",
      "Epoch 13/20\n",
      "9426/9426 [==============================] - 188s 20ms/step - loss: 0.9825 - output1_loss: 0.1519 - output2_loss: 0.1585 - output3_loss: 0.1621 - output4_loss: 0.1644 - output5_loss: 0.1683 - output6_loss: 0.1774\n",
      "Epoch 14/20\n",
      "9426/9426 [==============================] - 191s 20ms/step - loss: 0.9497 - output1_loss: 0.1472 - output2_loss: 0.1532 - output3_loss: 0.1563 - output4_loss: 0.1585 - output5_loss: 0.1622 - output6_loss: 0.1722\n",
      "Epoch 15/20\n",
      "9426/9426 [==============================] - 187s 20ms/step - loss: 0.9216 - output1_loss: 0.1431 - output2_loss: 0.1485 - output3_loss: 0.1516 - output4_loss: 0.1537 - output5_loss: 0.1573 - output6_loss: 0.16743s - loss: 0.9206 - output1_loss: 0.1430 - output2_loss: 0.1482 - output3_loss: 0.1514 - ou\n",
      "Epoch 16/20\n",
      "9426/9426 [==============================] - 186s 20ms/step - loss: 0.8934 - output1_loss: 0.1392 - output2_loss: 0.1439 - output3_loss: 0.1467 - output4_loss: 0.1487 - output5_loss: 0.1525 - output6_loss: 0.1625\n",
      "Epoch 17/20\n",
      "9426/9426 [==============================] - 184s 19ms/step - loss: 0.8691 - output1_loss: 0.1358 - output2_loss: 0.1400 - output3_loss: 0.1422 - output4_loss: 0.1440 - output5_loss: 0.1483 - output6_loss: 0.1587\n",
      "Epoch 18/20\n",
      "9426/9426 [==============================] - 185s 20ms/step - loss: 0.8465 - output1_loss: 0.1323 - output2_loss: 0.1363 - output3_loss: 0.1387 - output4_loss: 0.1401 - output5_loss: 0.1445 - output6_loss: 0.1546\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9426/9426 [==============================] - 183s 19ms/step - loss: 0.8242 - output1_loss: 0.1290 - output2_loss: 0.1329 - output3_loss: 0.1349 - output4_loss: 0.1360 - output5_loss: 0.1407 - output6_loss: 0.1507\n",
      "Epoch 20/20\n",
      "9426/9426 [==============================] - 184s 20ms/step - loss: 0.8037 - output1_loss: 0.1260 - output2_loss: 0.1293 - output3_loss: 0.1310 - output4_loss: 0.1328 - output5_loss: 0.1371 - output6_loss: 0.14756s - loss: 0.8019 - output1_loss: 0.1256 - \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25023423f70>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "model_input = layers.Input((SEQ_LEN-1, 6*(len(tab_dict)+1)))\n",
    "\n",
    "m = layers.LSTM(256, return_sequences=True)(model_input)\n",
    "m = layers.LSTM(256)(m)\n",
    "m = layers.Dense(512)(m)\n",
    "m = layers.LeakyReLU(0.3)(m)\n",
    "m = layers.Dense(512)(m)\n",
    "m = layers.LeakyReLU(0.3)(m)\n",
    "out = layers.Dense((len(tab_dict)+1), activation='softmax', name='output1')(m)\n",
    "out2 = layers.Dense((len(tab_dict)+1), activation='softmax', name='output2')(m)\n",
    "out3 = layers.Dense((len(tab_dict)+1), activation='softmax', name='output3')(m)\n",
    "out4 = layers.Dense((len(tab_dict)+1), activation='softmax', name='output4')(m)\n",
    "out5 = layers.Dense((len(tab_dict)+1), activation='softmax', name='output5')(m)\n",
    "out6 = layers.Dense((len(tab_dict)+1), activation='softmax', name='output6')(m)\n",
    "\n",
    "\n",
    "model = tf.keras.Model(inputs=model_input, outputs=[out,out2,out3,out4,out5,out6], name=\"tab_generator\")\n",
    "model.summary()\n",
    "\n",
    "gen = TabGenerator()\n",
    "\n",
    "class_weights = {} #set class weights:\n",
    "for i in range(CHAR_NUM): class_weights[i] = 1\n",
    "class_weights[1] = 0.05\n",
    "#Class weights are not supported for multiple outputs. Will need to make a custom loss function.\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy')\n",
    "model.fit(gen, epochs=20)#, validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "00de92a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "9426/9426 [==============================] - 185s 20ms/step - loss: 0.7831 - output1_loss: 0.1231 - output2_loss: 0.1259 - output3_loss: 0.1278 - output4_loss: 0.1290 - output5_loss: 0.1333 - output6_loss: 0.14400s - loss: 0.7831 - output1_loss: 0.1231 - output2_loss: 0.1259 - output3_loss: 0.1278 - output4_loss: 0.1290 - output5_loss: 0.1333 - output6_loss: 0.144\n",
      "Epoch 2/25\n",
      "9426/9426 [==============================] - 185s 20ms/step - loss: 0.7659 - output1_loss: 0.1205 - output2_loss: 0.1229 - output3_loss: 0.1247 - output4_loss: 0.1262 - output5_loss: 0.1306 - output6_loss: 0.1410\n",
      "Epoch 3/25\n",
      "9426/9426 [==============================] - 187s 20ms/step - loss: 0.7494 - output1_loss: 0.1182 - output2_loss: 0.1205 - output3_loss: 0.1215 - output4_loss: 0.1237 - output5_loss: 0.1275 - output6_loss: 0.1380\n",
      "Epoch 4/25\n",
      "9426/9426 [==============================] - 184s 20ms/step - loss: 0.7324 - output1_loss: 0.1159 - output2_loss: 0.1175 - output3_loss: 0.1188 - output4_loss: 0.1203 - output5_loss: 0.1246 - output6_loss: 0.1352\n",
      "Epoch 5/25\n",
      "9426/9426 [==============================] - 186s 20ms/step - loss: 0.7172 - output1_loss: 0.1132 - output2_loss: 0.1151 - output3_loss: 0.1163 - output4_loss: 0.1176 - output5_loss: 0.1221 - output6_loss: 0.1329\n",
      "Epoch 6/25\n",
      "9426/9426 [==============================] - 184s 20ms/step - loss: 0.7035 - output1_loss: 0.1115 - output2_loss: 0.1126 - output3_loss: 0.1140 - output4_loss: 0.1153 - output5_loss: 0.1198 - output6_loss: 0.1302\n",
      "Epoch 7/25\n",
      "9426/9426 [==============================] - 184s 19ms/step - loss: 0.6899 - output1_loss: 0.1097 - output2_loss: 0.1106 - output3_loss: 0.1117 - output4_loss: 0.1127 - output5_loss: 0.1172 - output6_loss: 0.1281\n",
      "Epoch 8/25\n",
      "9426/9426 [==============================] - 183s 19ms/step - loss: 0.6774 - output1_loss: 0.1070 - output2_loss: 0.1087 - output3_loss: 0.1095 - output4_loss: 0.1110 - output5_loss: 0.1153 - output6_loss: 0.1259\n",
      "Epoch 9/25\n",
      "9426/9426 [==============================] - 184s 19ms/step - loss: 0.6676 - output1_loss: 0.1058 - output2_loss: 0.1073 - output3_loss: 0.1075 - output4_loss: 0.1092 - output5_loss: 0.1134 - output6_loss: 0.1244\n",
      "Epoch 10/25\n",
      "9426/9426 [==============================] - 183s 19ms/step - loss: 0.6546 - output1_loss: 0.1039 - output2_loss: 0.1047 - output3_loss: 0.1058 - output4_loss: 0.1070 - output5_loss: 0.1112 - output6_loss: 0.1220\n",
      "Epoch 11/25\n",
      "9426/9426 [==============================] - 183s 19ms/step - loss: 0.6440 - output1_loss: 0.1021 - output2_loss: 0.1030 - output3_loss: 0.1043 - output4_loss: 0.1053 - output5_loss: 0.1091 - output6_loss: 0.120117s - loss: 0.6406 - output1_loss: 0.1016 - outpu - ETA: 8s\n",
      "Epoch 12/25\n",
      "9426/9426 [==============================] - 184s 20ms/step - loss: 0.6317 - output1_loss: 0.1002 - output2_loss: 0.1009 - output3_loss: 0.1021 - output4_loss: 0.1033 - output5_loss: 0.1074 - output6_loss: 0.1179\n",
      "Epoch 13/25\n",
      "9426/9426 [==============================] - 184s 20ms/step - loss: 0.6215 - output1_loss: 0.0992 - output2_loss: 0.0994 - output3_loss: 0.1005 - output4_loss: 0.1013 - output5_loss: 0.1051 - output6_loss: 0.1161\n",
      "Epoch 14/25\n",
      "9426/9426 [==============================] - 183s 19ms/step - loss: 0.6109 - output1_loss: 0.0972 - output2_loss: 0.0976 - output3_loss: 0.0982 - output4_loss: 0.0995 - output5_loss: 0.1040 - output6_loss: 0.1143\n",
      "Epoch 15/25\n",
      "9426/9426 [==============================] - 192s 20ms/step - loss: 0.6026 - output1_loss: 0.0962 - output2_loss: 0.0966 - output3_loss: 0.0968 - output4_loss: 0.0982 - output5_loss: 0.1023 - output6_loss: 0.1125\n",
      "Epoch 16/25\n",
      "9426/9426 [==============================] - 178s 19ms/step - loss: 0.5926 - output1_loss: 0.0944 - output2_loss: 0.0946 - output3_loss: 0.0953 - output4_loss: 0.0969 - output5_loss: 0.1006 - output6_loss: 0.1108\n",
      "Epoch 17/25\n",
      "9426/9426 [==============================] - 183s 19ms/step - loss: 0.5851 - output1_loss: 0.0931 - output2_loss: 0.0935 - output3_loss: 0.0943 - output4_loss: 0.0953 - output5_loss: 0.0994 - output6_loss: 0.1095\n",
      "Epoch 18/25\n",
      "9426/9426 [==============================] - 183s 19ms/step - loss: 0.5786 - output1_loss: 0.0923 - output2_loss: 0.0931 - output3_loss: 0.0935 - output4_loss: 0.0936 - output5_loss: 0.0978 - output6_loss: 0.1082\n",
      "Epoch 19/25\n",
      "9426/9426 [==============================] - 184s 19ms/step - loss: 0.5782 - output1_loss: 0.0921 - output2_loss: 0.0934 - output3_loss: 0.0929 - output4_loss: 0.0940 - output5_loss: 0.0977 - output6_loss: 0.10809s - loss: 0.5765 - output1_loss: 0.0917 - output2_loss: 0 - ETA: 3s - loss: 0.5776 - output1_loss: 0.0919 - output2_loss: 0.0932 - output3_loss: 0.0928 - output4_loss: 0.0940 - output5_loss: 0.0977 - o - ETA: 2s - loss: 0.5778 - output1_loss: 0.0920 - output2_loss: 0.0933 - output3_loss: 0.0928 - output4_loss: 0.0940 -\n",
      "Epoch 20/25\n",
      "9426/9426 [==============================] - 180s 19ms/step - loss: 0.5626 - output1_loss: 0.0898 - output2_loss: 0.0897 - output3_loss: 0.0905 - output4_loss: 0.0920 - output5_loss: 0.0952 - output6_loss: 0.1054\n",
      "Epoch 21/25\n",
      "9426/9426 [==============================] - 180s 19ms/step - loss: 0.5553 - output1_loss: 0.0886 - output2_loss: 0.0891 - output3_loss: 0.0890 - output4_loss: 0.0902 - output5_loss: 0.0940 - output6_loss: 0.1043\n",
      "Epoch 22/25\n",
      "9426/9426 [==============================] - 178s 19ms/step - loss: 0.5482 - output1_loss: 0.0872 - output2_loss: 0.0878 - output3_loss: 0.0882 - output4_loss: 0.0892 - output5_loss: 0.0928 - output6_loss: 0.1029\n",
      "Epoch 23/25\n",
      "9426/9426 [==============================] - 179s 19ms/step - loss: 0.5425 - output1_loss: 0.0864 - output2_loss: 0.0865 - output3_loss: 0.0871 - output4_loss: 0.0883 - output5_loss: 0.0925 - output6_loss: 0.1017\n",
      "Epoch 24/25\n",
      "9426/9426 [==============================] - 179s 19ms/step - loss: 0.5339 - output1_loss: 0.0849 - output2_loss: 0.0855 - output3_loss: 0.0858 - output4_loss: 0.0869 - output5_loss: 0.0904 - output6_loss: 0.1004\n",
      "Epoch 25/25\n",
      "9426/9426 [==============================] - 178s 19ms/step - loss: 0.5321 - output1_loss: 0.0847 - output2_loss: 0.0850 - output3_loss: 0.0854 - output4_loss: 0.0862 - output5_loss: 0.0909 - output6_loss: 0.1000\n",
      "--05-/~315~5x145xO6)23)2O2-7OxOxO1~O\n",
      "--1----5-33-7~25/56O-3O3--1x(~-3717-\n",
      "--6-3-1--31--024-33043)3--0-0-53737-\n",
      "-60-0----3---1-4-5---3-0--3-1-x35-4~\n",
      "-40------5-5-----~--------3---~0---r\n",
      "-)------/----1---~--------5---~-~-s3\n"
     ]
    }
   ],
   "source": [
    "model.fit(gen, epochs=25)#, validation_data=(x_test,y_test))\n",
    "\n",
    "predictions = []\n",
    "data = gen[18][0][5]\n",
    "for i in range(36):\n",
    "    prediction = model.predict(data.reshape(1, SEQ_LEN-1, 6*CHAR_NUM)) #prediction shape: (6, 1 ,chars) 6* [[chars]]\n",
    "    text_prediction = [seq_dict[np.argmax(s)] for s in prediction]\n",
    "    predictions.append(text_prediction)\n",
    "    prediction = np.array(prediction)\n",
    "    prediction = prediction.reshape(1, -1) #(1,6, chars) \n",
    "    data[:-1] = data[1:]; data[-1] = prediction\n",
    "\n",
    "predictions = np.array(predictions)\n",
    "for i in range(6):\n",
    "    print(''.join(predictions[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "550fe423",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_25_layer_call_and_return_conditional_losses, lstm_cell_25_layer_call_fn, lstm_cell_26_layer_call_and_return_conditional_losses, lstm_cell_26_layer_call_fn, lstm_cell_25_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('checkpoints/model1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a8d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
